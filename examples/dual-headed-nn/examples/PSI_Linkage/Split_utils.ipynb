{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "inclusive-regression",
   "metadata": {},
   "source": [
    "## Utility functions to Split MNIST data in two parts: bottom and top halves\n",
    "\n",
    "This code saves the images, the labels, and assigns indexes to the single samples. The indexes are necessary for linkage of the two halves with PSI (Private Set Intersection). The two different part of the images will be distributed to Data Owners. \n",
    "\n",
    "The Data Scientist will hold labels and will link the labels with the respective halves. In such a way, the Data Scientist would then be able to link the two parts and run the remote segment of the Split Neural Network on it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "confirmed-essence",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import syft as sy\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from typing import List, Tuple\n",
    "from torch.utils.data import SequentialSampler, RandomSampler, BatchSampler\n",
    "\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "refined-jerusalem",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(dataset, worker_list=None, n_workers=2):\n",
    "\n",
    "    if worker_list is None:\n",
    "        worker_list = list(range(0, n_workers))\n",
    "            \n",
    "    #counter to create the index of different data samples\n",
    "    idx = 0 \n",
    "    \n",
    "    #dictionary to accomodate the split data\n",
    "    dic_single_datasets = {}\n",
    "    for worker in worker_list: \n",
    "        \"\"\"\n",
    "        Each value is a list of three elements, to accomodate, in order: \n",
    "        - data examples (as tensors)\n",
    "        - label\n",
    "        - index \n",
    "        \"\"\"\n",
    "        dic_single_datasets[worker] = [] \n",
    "\n",
    "    \"\"\"\n",
    "    Loop through the dataset to split the data and labels vertically to assign to data owners. \n",
    "\n",
    "    \"\"\"\n",
    "    label_list = []\n",
    "    index_list = []\n",
    "    for tensor, label in dataset: \n",
    "        height = tensor.shape[-1]//len(worker_list)\n",
    "        i = 0\n",
    "        for worker in worker_list[:-1]: \n",
    "            dic_single_datasets[worker].append(tensor[:, :, height * i : height * (i + 1)])\n",
    "            i += 1\n",
    "            \n",
    "        #add the value of the last data owner / split\n",
    "        dic_single_datasets[worker_list[-1]].append(tensor[:, :, height * (i) : ])\n",
    "        label_list.append(torch.Tensor([label]))\n",
    "        index_list.append(torch.Tensor([idx]))\n",
    "        \n",
    "        idx += 1\n",
    "        \n",
    "    return dic_single_datasets, label_list, index_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "disabled-officer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to mnist/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.1%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting mnist/MNIST/raw/train-images-idx3-ubyte.gz to mnist/MNIST/raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to mnist/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "113.5%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting mnist/MNIST/raw/train-labels-idx1-ubyte.gz to mnist/MNIST/raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to mnist/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.4%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting mnist/MNIST/raw/t10k-images-idx3-ubyte.gz to mnist/MNIST/raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to mnist/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "180.4%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting mnist/MNIST/raw/t10k-labels-idx1-ubyte.gz to mnist/MNIST/raw\n",
      "Processing...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/daler/anaconda3/envs/syft4/lib/python3.7/site-packages/torchvision/datasets/mnist.py:480: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                              transforms.Normalize((0.5,), (0.5,)),\n",
    "                              ])\n",
    "trainset = datasets.MNIST('mnist', download=True, train=True, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "identified-gross",
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_single_datasets, label_list, index_list = split_data(trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "electronic-lecture",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "file = \"data_file/1st_owner\"\n",
    "pickle.dump(dic_single_datasets[0], open(file, 'wb'))\n",
    "\n",
    "file = \"data_file/2nd_owner\"\n",
    "pickle.dump(dic_single_datasets[1], open(file, 'wb'))\n",
    "\n",
    "file = \"data_file/indexlist\"\n",
    "pickle.dump(index_list, open(file, 'wb'))\n",
    "\n",
    "file = \"data_file/labellist\"\n",
    "pickle.dump(label_list, open(file, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "altered-medicaid",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "refined-investigator",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "clean-annex",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
